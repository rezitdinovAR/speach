{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibD6bsRPl8Qu"
   },
   "source": [
    "# Building an end-to-end Speech Recognition model in PyTorch - [AssemblyAI](https://www.assemblyai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1fXgsDQmK09"
   },
   "source": [
    "## installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwfN8o17Bdp2",
    "outputId": "d6cb1926-fc3c-4ac7-b551-1eaf69c04c0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install comet-ml==3.0.2 -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-05 09:51:37,712] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from comet_ml import Experiment\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "from dataset import IPS1ASRDataset\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSKHvy8DmOCQ"
   },
   "source": [
    "## Setting up your data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_arr = [\n",
    "            '<SPACE>', 'а', 'ә', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'җ', 'з', 'и', 'й', 'к', 'л',\n",
    "            'м', 'н', 'ң', 'о', 'ө', 'п', 'р', 'с', 'т', 'у', 'ү', 'ф', 'х', 'һ', 'ц', 'ч', 'ш', 'щ',\n",
    "            'ъ', 'ы', 'ь', 'э', 'ю', 'я'\n",
    "        ]\n",
    "\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for index in range(len(char_map_arr)):\n",
    "            ch = char_map_arr[index]\n",
    "            self.char_map[ch] = index\n",
    "            self.index_map[index] = ch\n",
    "        self.index_map[0] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to \n",
    "        an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            elif c in self.char_map:\n",
    "                ch = self.char_map[c]\n",
    "            else: \n",
    "                continue\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to \n",
    "        an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asr/miniconda3/envs/asr/lib/python3.11/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        utterance = utterance.lstrip('\\ufeff')\n",
    "        \n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVJs4Bk8FjjO",
    "outputId": "e17f3aa9-5973-4f22-af9d-43b6fff89e3a"
   },
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XdSlhAQnDEA"
   },
   "source": [
    "## The Model\n",
    "Base of of Deep Speech 2 with some personal improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "65H1-PCjm-FB"
   },
   "outputs": [],
   "source": [
    "class SpeechRecognitionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuguNEzKnMOn"
   },
   "source": [
    "## The Training and Evaluating Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment, accelerator):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    with experiment.train():\n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            \n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "            \n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
    "            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            iter_meter.step()\n",
    "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(spectrograms), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, epoch, iter_meter, experiment, accelerator):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with experiment.test():\n",
    "        with torch.no_grad():\n",
    "            for i, _data in enumerate(test_loader):\n",
    "                spectrograms, labels, input_lengths, label_lengths = _data\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "    avg_cer = sum(test_cer) / len(test_cer)\n",
    "    avg_wer = sum(test_wer) / len(test_wer)\n",
    "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
    "    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
    "    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(\n",
    "        test_loss, avg_cer, avg_wer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qBGdkQSmW3a"
   },
   "source": [
    "## Setting up Comet\n",
    "If you have a comet account, fill in teh api key, project name and experiment name below. You can create an account at [comet.ml](comet.ml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxRIb_WempDq"
   },
   "source": [
    "## GPU runtime\n",
    "If you are using a GPU runtime, this will let you know what GPU and how much memory is available. Adjust your batch_size depending on which GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlUSuAJwlzo8",
    "outputId": "9d01ebfc-4853-4a00-c51f-d0685f9abe80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  5 09:51:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    37W / 250W |   2586MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    31W / 250W |      4MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2546      C   /usr/bin/python3                 2582MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXvlWZeVpXfX"
   },
   "source": [
    "## Train\n",
    "this will download the data on first run and may take a while.\n",
    "\n",
    "If you have Comet.ml setup, you can start seeing your progress in the comet cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    comet_api_key = \"GUmMcuBnNsVBJjnslfRGmqKkI\" # add your api key here\n",
    "    project_name = \"TatAsr\"\n",
    "    experiment_name = \"TatAsr-cnn-rnn-accelerator-2gpu\"\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=comet_api_key, \n",
    "        project_name=project_name, \n",
    "        parse_args=False, \n",
    "        log_code=True, \n",
    "        auto_output_logging=\"default\"\n",
    "    )\n",
    "\n",
    "    experiment.set_name(experiment_name)\n",
    "\n",
    "    set_seed(42)\n",
    "    torch.manual_seed(7)\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    ips_dataset_train = IPS1ASRDataset('../tatar_tts/train/')\n",
    "    ips_dataset_valid = IPS1ASRDataset('../tatar_tts/valid/')\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    learning_rate *= 2\n",
    "    batch_size = 128\n",
    "    epochs = 30\n",
    "    \n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 40, # Длина алфавита\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], \n",
    "        hparams['n_rnn_layers'], \n",
    "        hparams['rnn_dim'],\n",
    "        hparams['n_class'], \n",
    "        hparams['n_feats'], \n",
    "        hparams['stride'], \n",
    "        hparams['dropout']\n",
    "    )\n",
    "\n",
    "    experiment.log_parameters(hparams)\n",
    "    \n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        dataset=ips_dataset_train,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: data_processing(x, 'train'),\n",
    "    )\n",
    "    \n",
    "    test_loader = data.DataLoader(\n",
    "        dataset=ips_dataset_valid,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=39, zero_infinity=True) # Длина алфавита - 1\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=hparams['learning_rate'],\n",
    "        steps_per_epoch=int(len(train_loader)),\n",
    "        epochs=hparams['epochs'],\n",
    "        anneal_strategy='linear'\n",
    "    )\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    \n",
    "    model, optimizer, train_loader, test_loader, scheduler, experiment = accelerator.prepare(\n",
    "        model, optimizer, train_loader, test_loader, scheduler, experiment\n",
    "    )\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment, accelerator)\n",
    "        test(model, test_loader, criterion, epoch, iter_meter, experiment, accelerator)\n",
    "        accelerator.save_model(model, f'./models/TatAsr-1-accelerator-epoch-1-{epoch}')\n",
    "    \n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XZodve8PGKfS",
    "outputId": "d1448327-81d3-4b68-8854-8223a5bcadbc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/gumaonelove/tatasr/5c5cfbf1114442d99fcbce45a00bd2a3\n",
      "\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/gumaonelove/tatasr/9d9d00c29ac7403ca87d06307f9d5ecb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3173 (0%)]\tLoss: 26.527718Train Epoch: 1 [0/3173 (0%)]\tLoss: 27.869038\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 4.9125, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 4.9383, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 2 [0/3173 (0%)]\tLoss: 4.821369Train Epoch: 2 [0/3173 (0%)]\tLoss: 4.840461\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.9383, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.9241, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 3 [0/3173 (0%)]\tLoss: 3.747259\n",
      "Train Epoch: 3 [0/3173 (0%)]\tLoss: 3.822659\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6178, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Test set: Average loss: 3.6187, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 4 [0/3173 (0%)]\tLoss: 3.608856Train Epoch: 4 [0/3173 (0%)]\tLoss: 3.573582\n",
      "\n",
      "\n",
      "evaluating...\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.6342, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6366, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 5 [0/3173 (0%)]\tLoss: 3.560077Train Epoch: 5 [0/3173 (0%)]\tLoss: 3.540582\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6116, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6209, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 6 [0/3173 (0%)]\tLoss: 3.557018\n",
      "Train Epoch: 6 [0/3173 (0%)]\tLoss: 3.589831\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6255, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Test set: Average loss: 3.6244, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 7 [0/3173 (0%)]\tLoss: 3.566354Train Epoch: 7 [0/3173 (0%)]\tLoss: 3.613182\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6242, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6290, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 8 [0/3173 (0%)]\tLoss: 3.599092Train Epoch: 8 [0/3173 (0%)]\tLoss: 3.536698\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6135, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6192, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 9 [0/3173 (0%)]\tLoss: 3.599344Train Epoch: 9 [0/3173 (0%)]\tLoss: 3.487879\n",
      "\n",
      "\n",
      "evaluating...\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.6035, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6110, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 10 [0/3173 (0%)]\tLoss: 3.646652Train Epoch: 10 [0/3173 (0%)]\tLoss: 3.610108\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6882, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6860, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 11 [0/3173 (0%)]\tLoss: 3.637599Train Epoch: 11 [0/3173 (0%)]\tLoss: 3.534245\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6451, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6466, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 12 [0/3173 (0%)]\tLoss: 3.566220\n",
      "Train Epoch: 12 [0/3173 (0%)]\tLoss: 3.601652\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6614, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6616, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 13 [0/3173 (0%)]\tLoss: 3.588985Train Epoch: 13 [0/3173 (0%)]\tLoss: 3.514062\n",
      "\n",
      "\n",
      "evaluating...\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.6202, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6260, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 14 [0/3173 (0%)]\tLoss: 3.595125Train Epoch: 14 [0/3173 (0%)]\tLoss: 3.602815\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6348, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6355, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 15 [0/3173 (0%)]\tLoss: 3.502931Train Epoch: 15 [0/3173 (0%)]\tLoss: 3.591350\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.6396, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6395, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 16 [0/3173 (0%)]\tLoss: 3.570524Train Epoch: 16 [0/3173 (0%)]\tLoss: 3.409200\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5985, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.6036, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 17 [0/3173 (0%)]\tLoss: 3.554509Train Epoch: 17 [0/3173 (0%)]\tLoss: 3.521625\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5718, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5746, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 18 [0/3173 (0%)]\tLoss: 3.611772Train Epoch: 18 [0/3173 (0%)]\tLoss: 3.493715\n",
      "\n",
      "\n",
      "evaluating...\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.5512, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5460, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 19 [0/3173 (0%)]\tLoss: 3.516479\n",
      "Train Epoch: 19 [0/3173 (0%)]\tLoss: 3.562305\n",
      "\n",
      "evaluating...\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.5404, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5378, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 20 [0/3173 (0%)]\tLoss: 3.549814Train Epoch: 20 [0/3173 (0%)]\tLoss: 3.499745\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5585, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5574, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 21 [0/3173 (0%)]\tLoss: 3.489503\n",
      "Train Epoch: 21 [0/3173 (0%)]\tLoss: 3.500913\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5518, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5504, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 22 [0/3173 (0%)]\tLoss: 3.560680Train Epoch: 22 [0/3173 (0%)]\tLoss: 3.506075\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5276, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5317, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 23 [0/3173 (0%)]\tLoss: 3.471834Train Epoch: 23 [0/3173 (0%)]\tLoss: 3.457101\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5367, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Test set: Average loss: 3.5342, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 24 [0/3173 (0%)]\tLoss: 3.477010Train Epoch: 24 [0/3173 (0%)]\tLoss: 3.504329\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5269, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5317, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 25 [0/3173 (0%)]\tLoss: 3.390831\n",
      "Train Epoch: 25 [0/3173 (0%)]\tLoss: 3.476448\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5349, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Test set: Average loss: 3.5362, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 26 [0/3173 (0%)]\tLoss: 3.497452\n",
      "Train Epoch: 26 [0/3173 (0%)]\tLoss: 3.432862\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5234, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5277, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 27 [0/3173 (0%)]\tLoss: 3.446314Train Epoch: 27 [0/3173 (0%)]\tLoss: 3.524974\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5210, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5266, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 28 [0/3173 (0%)]\tLoss: 3.485987Train Epoch: 28 [0/3173 (0%)]\tLoss: 3.449104\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5215, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5243, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 29 [0/3173 (0%)]\tLoss: 3.429778Train Epoch: 29 [0/3173 (0%)]\tLoss: 3.442404\n",
      "\n",
      "\n",
      "evaluating...\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.5205, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n",
      "Test set: Average loss: 3.5239, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Train Epoch: 30 [0/3173 (0%)]\tLoss: 3.431833Train Epoch: 30 [0/3173 (0%)]\tLoss: 3.436439\n",
      "\n",
      "\n",
      "evaluating...\n",
      "evaluating...\n",
      "\n",
      "Test set: Average loss: 3.5240, Average CER: 0.988035 Average WER: 0.9990\n",
      "\n",
      "Test set: Average loss: 3.5208, Average CER: 0.988174 Average WER: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(training_function, num_processes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucfQX3qN21az"
   },
   "source": [
    "## Результаты обучения\n",
    "### Модель \n",
    "Использовалась модель из статьи [Building an End-to-End Speech Recognition Model in PyTorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/) **Deep Speech 2**. \n",
    "\n",
    "### Гипер параметры\n",
    "* `n_cnn_layers` = 3\n",
    "* `n_rnn_layers` = 5\n",
    "* `rnn_dim` = 512\n",
    "* `n_class` = 29\n",
    "* `n_feats` = 128\n",
    "* `stride` = 2\n",
    "* `dropout` = 0.1\n",
    "* `learning_rate` = 0.001\n",
    "* `batch_size` = 80\n",
    "* `epochs` = 10\n",
    "* `num_workers` = 8\n",
    "\n",
    "### Метрики\n",
    "Ниже будут приведены средний \n",
    "* `CER` - частота ошибок в символах\n",
    "* `WER` - частота ошибок в словах\n",
    "\n",
    "Формула расчета\n",
    "**WER** = (S+D+I)/N = (S+D+I)/(S+D+C), где:\n",
    "\n",
    "* **S** — количество замен\n",
    "* **D** — количество удалений\n",
    "* **I** — количество вставок\n",
    "* **C** — количество корректных слов\n",
    "* **N** — количество слов в исходной строке\n",
    "\n",
    "Итоговык метрики \n",
    "* `CER` = 0.24 \n",
    "* `WER` = 0.80\n",
    "\n",
    "### Выводы\n",
    "1. Обучать ASR на одной видео карте **tesla v100** вышеупомянутую модель 2.5 часа, из-за этого проблематично тестировать гипотезы и количество слоев в данной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
