{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e9230b",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8110965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa482e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asr/miniconda3/envs/asr/lib/python3.11/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "PATH = '../'\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"anton-l/wav2vec2-large-xlsr-53-tatar\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"anton-l/wav2vec2-large-xlsr-53-tatar\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d91c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"audio\"]['path'])\n",
    "    batch[\"speech\"] = speech_array.squeeze().numpy()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a20057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0051adc771f54e5ab4820d1cf0dd18b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/146372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5187b79bcab3477094dbfcb493913ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_1 = load_dataset(\"audiofolder\", data_dir=PATH + 'tatar_asr_1')\n",
    "# dataset_2 = load_dataset(\"audiofolder\", data_dir=PATH + 'tatar_asr_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c676b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'Unnamed: 0', 'transcription'],\n",
       "        num_rows: 73185\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'Unnamed: 0', 'transcription'],\n",
       "        num_rows: 14253\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd20da82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/home/asr/projects/speach/tatar_asr_1/test/20.1.wav',\n",
       " 'array': array([ 2.13623047e-04,  6.10351562e-05, -3.96728516e-04, ...,\n",
       "         3.35693359e-04,  3.96728516e-04,  2.13623047e-04]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1['test']['audio'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41a6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = dataset_1.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aba66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(dataset_1['test'][\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e8385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values.to('cuda'), attention_mask=inputs.attention_mask.to('cuda')).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91f1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96652a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['шедөният зехнән килеханын сорады', 'сүзләр вәкдәләшеләр төммуена барды']\n",
      "Reference: ['фидания цехның телефонын сорады', 'сүзләр вәгъдәләшүләр төн буена барды']\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction:\", processor.batch_decode(predicted_ids))\n",
    "print(\"Reference:\", dataset_1['test'][\"transcription\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bc462",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f2f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jiwer -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e87ee526",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = load_metric(\"wer\")\n",
    "model.to(\"cuda\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab6f1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    # 'ё' is equivalent to 'е'\n",
    "    sent = sent.replace('ё', 'е')\n",
    "    # replace non-alpha characters with space\n",
    "    sent = \"\".join(ch if ch.isalpha() else \" \" for ch in sent)\n",
    "    # remove repeated spaces\n",
    "    sent = \" \".join(sent.split())\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ee1db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2dc2f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14253/14253 [11:16<00:00, 21.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(dataset_1['test'], total=len(dataset_1['test'])):\n",
    "    row[\"transcription\"] = clean_sentence(row[\"transcription\"])\n",
    "\n",
    "    inputs = processor(row[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.to('cuda'), attention_mask=inputs.attention_mask.to('cuda')).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    targets.append(row[\"transcription\"])\n",
    "    preds.append(processor.batch_decode(pred_ids)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0afed865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 47.310808\n"
     ]
    }
   ],
   "source": [
    "print(\"WER: {:2f}\".format(100 * wer.compute(predictions=preds, references=targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e438d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
