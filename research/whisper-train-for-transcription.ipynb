{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade comet_ml -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:38.118293Z",
     "iopub.status.busy": "2023-09-04T16:50:38.117992Z",
     "iopub.status.idle": "2023-09-04T16:50:50.571885Z",
     "shell.execute_reply": "2023-09-04T16:50:50.570848Z",
     "shell.execute_reply.started": "2023-09-04T16:50:38.118267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-05 23:53:27,899] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHISPER_MODEL examples:\n",
    "\n",
    "* 1) openai/whisper-small (suitable for testing functionality, not very accurate on sentences, but capable of recognizing individual words or phrases. Requires low computational resources)\n",
    "* 2) openai/whisper-medium (recommended medium model)\n",
    "* 3) openai/whisper-large (sufficiently accurate on large sentences, but requires significant computational resources)\n",
    "* 4) openai/whisper-large-v2 (sufficiently accurate on large sentences, but requires significant computational resources)\n",
    "* 5) lorenzoncina/whisper-medium-ru (a model finetuned on the Russian language - recommended for training on Russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:50.574473Z",
     "iopub.status.busy": "2023-09-04T16:50:50.573546Z",
     "iopub.status.idle": "2023-09-04T16:50:50.615610Z",
     "shell.execute_reply": "2023-09-04T16:50:50.614730Z",
     "shell.execute_reply.started": "2023-09-04T16:50:50.574437Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
    "\n",
    "WHISPER_MODEL = 'openai/whisper-small'\n",
    "DATASET_DIR = '/kaggle/input/it-spectrum-dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:50.619311Z",
     "iopub.status.busy": "2023-09-04T16:50:50.619037Z",
     "iopub.status.idle": "2023-09-04T16:51:32.900398Z",
     "shell.execute_reply": "2023-09-04T16:51:32.899357Z",
     "shell.execute_reply.started": "2023-09-04T16:50:50.619288Z"
    }
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(WHISPER_MODEL)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(WHISPER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:32.904246Z",
     "iopub.status.busy": "2023-09-04T16:51:32.903878Z",
     "iopub.status.idle": "2023-09-04T16:51:32.909408Z",
     "shell.execute_reply": "2023-09-04T16:51:32.908477Z",
     "shell.execute_reply.started": "2023-09-04T16:51:32.904212Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the model's language and defining the task of transcription\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"tatar\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training dataset. When indexed, it returns a list containing:\n",
    "\n",
    "* 1) filepath - path to the audio\n",
    "* 2) text - transcribed text by annotators\n",
    "* 3) input_features - audio features for prediction\n",
    "* 4) labels - transcribed text by annotators converted into tokens\n",
    "* 5) attention mask - an attention mask where each element indicates whether the model should pay attention to the token corresponding to the same index in the labels list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.307664Z",
     "iopub.status.busy": "2023-09-04T16:51:33.307288Z",
     "iopub.status.idle": "2023-09-04T16:51:33.317645Z",
     "shell.execute_reply": "2023-09-04T16:51:33.316636Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.307616Z"
    }
   },
   "outputs": [],
   "source": [
    "class WhisperDataset(Dataset):\n",
    "    def __init__(self, audio_dir: str, processor, max_length, only_char=True):\n",
    "        self.audio_dir = audio_dir\n",
    "        df = pd.read_csv(audio_dir[:-1] + '.csv', index_col='id')\n",
    "        self.data = {}\n",
    "        counter = 0\n",
    "        for row in df.itertuples():\n",
    "            if not os.path.exists(audio_dir + str(row[0]) + '.txt'):\n",
    "                print('Отсутствует файл', str(row[0]) + '.txt')\n",
    "                continue\n",
    "            if not os.path.exists(audio_dir + str(row[0]) + '.wav'):\n",
    "                print(f'Отсутствует файл', str(row[0]) + '.wav')\n",
    "                continue\n",
    "            self.data[counter] = {\n",
    "                'text': str(row[0]) + '.txt',\n",
    "                'audio': str(row[0]) + '.wav'\n",
    "            }\n",
    "            counter += 1\n",
    "        self.len = counter - 1\n",
    "        self.only_char = only_char\n",
    "        del counter\n",
    "        del df\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        return self.audio_dir + self.data[index]['audio']\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        label_path = self.audio_dir + self.data[index]['text']\n",
    "        with open(label_path, 'r') as f:\n",
    "            label = clean_text(f.read()) if self.only_char else f.read() # Не учитывает ошибки в заполнение .txt\n",
    "        return label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self._get_audio_sample_path(idx)\n",
    "        text = self._get_audio_sample_label(idx)\n",
    "        \n",
    "        audio, sample_rate = torchaudio.load(filepath)\n",
    "        audio = torch.reshape(audio, (-1,))\n",
    "        \n",
    "        tokenized = self.processor.tokenizer(\n",
    "            text, return_tensors='pt', padding='max_length', return_attention_mask=True, \n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        labels, attention_mask = tokenized['input_ids'][0], tokenized['attention_mask'][0]\n",
    "        \n",
    "        input_features = self.processor(audio, return_tensors=\"pt\", sampling_rate=sample_rate).input_features[0]\n",
    "        \n",
    "        return {\n",
    "            'input_features': input_features, \n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.332146Z",
     "iopub.status.busy": "2023-09-04T16:51:33.331814Z",
     "iopub.status.idle": "2023-09-04T16:51:33.338088Z",
     "shell.execute_reply": "2023-09-04T16:51:33.337238Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.332116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отсутствует файл 198.2.txt\n",
      "Отсутствует файл 241.2.txt\n",
      "Отсутствует файл 227.2.txt\n",
      "Отсутствует файл 272.51.txt\n",
      "Отсутствует файл 228.2.txt\n",
      "Отсутствует файл 224.2.txt\n",
      "Отсутствует файл 238.2.txt\n",
      "Отсутствует файл 202.2.txt\n"
     ]
    }
   ],
   "source": [
    "# create train/val/test datasets\n",
    "train_dataset = WhisperDataset('../tatar_tts/train/', processor, model.config.max_length)\n",
    "valid_dataset = WhisperDataset('../tatar_tts/valid/', processor, model.config.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.340120Z",
     "iopub.status.busy": "2023-09-04T16:51:33.339329Z",
     "iopub.status.idle": "2023-09-04T16:51:33.348751Z",
     "shell.execute_reply": "2023-09-04T16:51:33.347701Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.340081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3171, 1859)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  5 23:53:34 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    34W / 250W |   2250MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    27W / 250W |      4MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2546      C   /usr/bin/python3                 2246MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml.init( project_name = \"TatAsr-whisper\", experiment_name = \"TatAsr-cnn-rnn-accelerator-2gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    global model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./whisper', \n",
    "        overwrite_output_dir=True, \n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=6,\n",
    "        save_steps=500, \n",
    "        save_total_limit=2,\n",
    "        do_train=True,\n",
    "    )\n",
    "    \n",
    "    set_seed(42)\n",
    "    torch.manual_seed(7)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m You are trying to log string value as a metric. This is not recommended.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/gumaonelove/tatasr-whisper/29a11dd378584871ac4d598bec28ab79\n",
      "\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 06:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Directory ./whisper is empty; no files were uploaded.\n",
      "Please double-check the directory path and the recursive parameter\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/gumaonelove/tatasr-whisper/29a11dd378584871ac4d598bec28ab79\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch                    : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     total_flos               : 9.177015389677158e+17\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss               : 0.01927664594830207\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_runtime            : 415.7209\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_samples_per_second : 7.628\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_steps_per_second   : 0.637\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : TatAsr-cnn-rnn-accelerator-2gpu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_frozen                            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_n_gpu                             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_no_sync_in_gradient_accumulation  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_setup_devices                     : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adafactor                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta1                         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta2                         : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_epsilon                       : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/auto_find_batch_size               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16_full_eval                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/data_seed                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_drop_last               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_num_workers             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_pin_memory              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_backend                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_broadcast_buffers              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_bucket_cap_mb                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_find_unused_parameters         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout                        : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout_delta                  : 0:30:00\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/debug                              : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed_plugin                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/default_optim                      : adamw_torch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/device                             : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/disable_tqdm                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dispatch_batches                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/distributed_state                  : Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_eval                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_predict                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_train                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_accumulation_steps            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_batch_size                    : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_delay                         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_steps                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/evaluation_strategy                : IntervalStrategy.NO\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_backend                       : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_full_eval                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_opt_level                     : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/framework                          : pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp                               : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_config                        : {'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_min_num_params                : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_transformer_layer_cls_to_wrap : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/full_determinism                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_accumulation_steps        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/greater_is_better                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/group_by_length                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/half_precision_backend             : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_always_push                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_model_id                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_private_repo                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_strategy                       : HubStrategy.EVERY_SAVE\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_token                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ignore_data_skip                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_inputs_for_metrics         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/jit_mode_eval                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_names                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_smoothing_factor             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/learning_rate                      : 5e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/length_column_name                 : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/load_best_model_at_end             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_process_index                : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_rank                         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level                          : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level_replica                  : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_on_each_node                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_dir                        : ./whisper/runs/Sep05_23-53-36_msteam\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_first_step                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_nan_inf_filter             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_steps                      : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_strategy                   : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_type                  : SchedulerType.LINEAR\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_grad_norm                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_steps                          : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/metric_for_best_model              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/mp_parameters                      : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/n_gpu                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/no_cuda                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/num_train_epochs                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim                              : OptimizerNames.ADAMW_TORCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_args                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/output_dir                         : ./whisper\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/overwrite_output_dir               : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/parallel_mode                      : ParallelMode.DISTRIBUTED\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/past_index                         : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_eval_batch_size         : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_train_batch_size        : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_eval_batch_size            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_train_batch_size           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/place_model_on_device              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/prediction_loss_only               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/process_index                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_model_id               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_organization           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_token                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ray_scope                          : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/remove_unused_columns              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/report_to                          : ['comet_ml']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/resume_from_checkpoint             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/run_name                           : ./whisper\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_on_each_node                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_safetensors                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_steps                         : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_strategy                      : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_total_limit                   : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/seed                               : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/sharded_ddp                        : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_log                         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_save                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/skip_memory_metrics                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tf32                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_backend              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_mode                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torchdynamo                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_metrics_debug                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_num_cores                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/train_batch_size                   : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_cpu                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_ipex                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_legacy_prediction_loop         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_mps_device                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_ratio                       : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_steps                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/weight_decay                       : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/world_size                         : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_auto_class                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_commit_hash                     : 4b5b137daa212e76d53e2923d1bf5837d3a9adf4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_name_or_path                    : openai/whisper-small\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/activation_dropout               : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/activation_function              : gelu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/add_cross_attention              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/apply_spec_augment               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/architectures                    : ['WhisperForConditionalGeneration']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attention_dropout                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attribute_map                    : {'num_attention_heads': 'encoder_attention_heads', 'hidden_size': 'd_model'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bad_words_ids                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/begin_suppress_tokens            : [220, 50257]\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bos_token_id                     : 50257\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/chunk_size_feed_forward          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/classifier_proj_size             : 256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/cross_attention_hidden_size      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/d_model                          : 768\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_attention_heads          : 12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_ffn_dim                  : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_layerdrop                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_layers                   : 12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_start_token_id           : 50258\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/diversity_penalty                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/do_sample                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/dropout                          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/early_stopping                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_attention_heads          : 12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_ffn_dim                  : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_layerdrop                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_layers                   : 12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_no_repeat_ngram_size     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/eos_token_id                     : 50257\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/exponential_decay_length_penalty : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/finetuning_task                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_bos_token_id              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_decoder_ids               : [(1, 50351), (2, 50359), (3, 50363)]\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_eos_token_id              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/id2label                         : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/init_std                         : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_composition                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_decoder                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_encoder_decoder               : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/keys_to_ignore_at_inference      : ['past_key_values']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/label2id                         : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/length_penalty                   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_feature_length              : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_feature_min_masks           : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_feature_prob                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_time_length                 : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_time_min_masks              : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_time_prob                   : 0.05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_length                       : 448\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_source_positions             : 1500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_target_positions             : 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/median_filter_width              : 7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/min_length                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/model_type                       : whisper\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/name_or_path                     : openai/whisper-small\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/no_repeat_ngram_size             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beam_groups                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beams                        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_hidden_layers                : 12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_labels                       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_mel_bins                     : 80\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_return_sequences             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_attentions                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_hidden_states             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_scores                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pad_token_id                     : 50257\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/prefix                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/problem_type                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pruned_heads                     : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/remove_invalid_values            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/repetition_penalty               : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict                      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict_in_generate          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/scale_embedding                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sep_token_id                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/suppress_tokens                  : [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362]\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/task_specific_params             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/temperature                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tf_legacy_loss                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_encoder_decoder              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_word_embeddings              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tokenizer_class                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_k                            : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_p                            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torch_dtype                      : torch.float32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torchscript                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/transformers_version             : 4.27.0.dev0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/typical_p                        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_bfloat16                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_cache                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_return_dict                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_weighted_layer_sum           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_size                       : 51865\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (1.32 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(training_function, num_processes=2, mixed_precision='fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m experiment\u001b[38;5;241m.\u001b[39mend()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:47.126953Z",
     "iopub.status.busy": "2023-09-04T17:03:47.126564Z",
     "iopub.status.idle": "2023-09-04T17:03:47.132825Z",
     "shell.execute_reply": "2023-09-04T17:03:47.131944Z",
     "shell.execute_reply.started": "2023-09-04T17:03:47.126921Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# getting folder of the newest checkpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./whisper\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     do_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(training_args\u001b[38;5;241m.\u001b[39moutput_dir), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(training_args\u001b[38;5;241m.\u001b[39moutput_dir, checkpoint_path)\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# getting folder of the newest checkpoint\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./whisper', \n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=6,\n",
    "    save_steps=500, \n",
    "    save_total_limit=2,\n",
    "    do_train=True,\n",
    ")\n",
    "checkpoint_path = max(os.listdir(training_args.output_dir), key=lambda x: int(x.split('-')[-1]) if 'checkpoint-' in x else 0)\n",
    "checkpoint_path = os.path.join(training_args.output_dir, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:47.134928Z",
     "iopub.status.busy": "2023-09-04T17:03:47.134287Z",
     "iopub.status.idle": "2023-09-04T17:03:51.326360Z",
     "shell.execute_reply": "2023-09-04T17:03:51.325346Z",
     "shell.execute_reply.started": "2023-09-04T17:03:47.134896Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset: WhisperDataset) -> pd.DataFrame:\n",
    "    predicted_df = pd.DataFrame([], columns=['filename', 'pred', 'gt'])\n",
    "    for filepath, text, input_features, _, attention_mask in tqdm(test_dataset):\n",
    "        filename = filepath.replace('\\\\', '/').split('/')[-1]\n",
    "    \n",
    "        input_features = torch.stack([input_features]).to('cuda')\n",
    "        generated_ids = model.generate(inputs=input_features, attention_mask=attention_mask)\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "        predicted_df.loc[len(predicted_df)] = [filename, transcription, text]\n",
    "    return predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:51.328494Z",
     "iopub.status.busy": "2023-09-04T17:03:51.328121Z",
     "iopub.status.idle": "2023-09-04T17:03:54.221716Z",
     "shell.execute_reply": "2023-09-04T17:03:54.220635Z",
     "shell.execute_reply.started": "2023-09-04T17:03:51.328462Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_df = predict(model, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.223590Z",
     "iopub.status.busy": "2023-09-04T17:03:54.223138Z",
     "iopub.status.idle": "2023-09-04T17:03:54.234701Z",
     "shell.execute_reply": "2023-09-04T17:03:54.233633Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.223541Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.236945Z",
     "iopub.status.busy": "2023-09-04T17:03:54.236278Z",
     "iopub.status.idle": "2023-09-04T17:03:54.245320Z",
     "shell.execute_reply": "2023-09-04T17:03:54.244367Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.236910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# metric: accuracy\n",
    "acc = sum((predicted_df['pred'] == predicted_df['gt'])) / len(predicted_df)\n",
    "print(f'{acc * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom sample test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.247339Z",
     "iopub.status.busy": "2023-09-04T17:03:54.246977Z",
     "iopub.status.idle": "2023-09-04T17:03:54.257558Z",
     "shell.execute_reply": "2023-09-04T17:03:54.256699Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.247307Z"
    }
   },
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(os.path.join(DATASET_DIR, 'sample/eu.0124f456-13b8-4765-936a-36bfd483683e.wav'), sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.259598Z",
     "iopub.status.busy": "2023-09-04T17:03:54.259174Z",
     "iopub.status.idle": "2023-09-04T17:03:54.319455Z",
     "shell.execute_reply": "2023-09-04T17:03:54.318206Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.259545Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = processor(audio, return_tensors='pt', sampling_rate=sample_rate)\n",
    "input_features = inputs.input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.321383Z",
     "iopub.status.busy": "2023-09-04T17:03:54.321037Z",
     "iopub.status.idle": "2023-09-04T17:03:54.506578Z",
     "shell.execute_reply": "2023-09-04T17:03:54.505617Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.321352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(inputs=input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.508427Z",
     "iopub.status.busy": "2023-09-04T17:03:54.508090Z",
     "iopub.status.idle": "2023-09-04T17:03:54.516341Z",
     "shell.execute_reply": "2023-09-04T17:03:54.515232Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.508396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новая\n"
     ]
    }
   ],
   "source": [
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
