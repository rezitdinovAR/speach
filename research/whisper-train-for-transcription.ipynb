{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:38.118293Z",
     "iopub.status.busy": "2023-09-04T16:50:38.117992Z",
     "iopub.status.idle": "2023-09-04T16:50:50.571885Z",
     "shell.execute_reply": "2023-09-04T16:50:50.570848Z",
     "shell.execute_reply.started": "2023-09-04T16:50:38.118267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-04 19:16:52,898] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import librosa # for audio reading\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHISPER_MODEL examples:\n",
    "\n",
    "* 1) openai/whisper-small (suitable for testing functionality, not very accurate on sentences, but capable of recognizing individual words or phrases. Requires low computational resources)\n",
    "* 2) openai/whisper-medium (recommended medium model)\n",
    "* 3) openai/whisper-large (sufficiently accurate on large sentences, but requires significant computational resources)\n",
    "* 4) openai/whisper-large-v2 (sufficiently accurate on large sentences, but requires significant computational resources)\n",
    "* 5) lorenzoncina/whisper-medium-ru (a model finetuned on the Russian language - recommended for training on Russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:50.574473Z",
     "iopub.status.busy": "2023-09-04T16:50:50.573546Z",
     "iopub.status.idle": "2023-09-04T16:50:50.615610Z",
     "shell.execute_reply": "2023-09-04T16:50:50.614730Z",
     "shell.execute_reply.started": "2023-09-04T16:50:50.574437Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "os.environ['WANDB_DISABLED'] = 'true' # disable logging of wandb\n",
    "\n",
    "WHISPER_MODEL = 'openai/whisper-small'\n",
    "DATASET_DIR = '/kaggle/input/it-spectrum-dataset/'\n",
    "VAL_PERCENT, TEST_PERCENT = 0.05, 0.2 # dataset is divided into train, validation and test according to these values multiplied by 100%. TRAIN_PERCENT = 1 - (VAL_PERCENT + TEST_PERCENT)\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir='./whisper', # the directory to save checkpoints\n",
    "    overwrite_output_dir=True, # overwrite output directory if exists\n",
    "    num_train_epochs=10, # number of epochs. One epoch is a single pass through the entire dataset. The number of epochs to use depends on the size of the dataset. Too many epochs can lead to overfitting, which can be detected by monitoring the validation loss during training. Too few epochs can result in underfitting, which can be identified by a consistently \"sharp\" decrease in the loss.\n",
    "    per_device_train_batch_size=2, # The batch size per iteration on one GPU. It is ideally in the form of a power of two (2, 4, 8), but should not exceed 64 (using larger batch sizes can lead to worse results from the optimizer)\n",
    "    save_steps=500, # save checkpoint each X iterations\n",
    "    save_total_limit=2, # maximum number of checkpoints in a folder, where older checkpoints are deleted when new ones are saved\n",
    "    do_train=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:50.619311Z",
     "iopub.status.busy": "2023-09-04T16:50:50.619037Z",
     "iopub.status.idle": "2023-09-04T16:51:32.900398Z",
     "shell.execute_reply": "2023-09-04T16:51:32.899357Z",
     "shell.execute_reply.started": "2023-09-04T16:50:50.619288Z"
    }
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(WHISPER_MODEL)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(WHISPER_MODEL).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:32.904246Z",
     "iopub.status.busy": "2023-09-04T16:51:32.903878Z",
     "iopub.status.idle": "2023-09-04T16:51:32.909408Z",
     "shell.execute_reply": "2023-09-04T16:51:32.908477Z",
     "shell.execute_reply.started": "2023-09-04T16:51:32.904212Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the model's language and defining the task of transcription\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"tatar\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training dataset. When indexed, it returns a list containing:\n",
    "\n",
    "* 1) filepath - path to the audio\n",
    "* 2) text - transcribed text by annotators\n",
    "* 3) input_features - audio features for prediction\n",
    "* 4) labels - transcribed text by annotators converted into tokens\n",
    "* 5) attention mask - an attention mask where each element indicates whether the model should pay attention to the token corresponding to the same index in the labels list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.307664Z",
     "iopub.status.busy": "2023-09-04T16:51:33.307288Z",
     "iopub.status.idle": "2023-09-04T16:51:33.317645Z",
     "shell.execute_reply": "2023-09-04T16:51:33.316636Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.307616Z"
    }
   },
   "outputs": [],
   "source": [
    "class WhisperDataset(Dataset):\n",
    "    def __init__(self, audio_dir: str, processor, only_char=True):\n",
    "        self.audio_dir = audio_dir\n",
    "        df = pd.read_csv(audio_dir[:-1] + '.csv', index_col='id')\n",
    "        self.data = {}\n",
    "        counter = 0\n",
    "        for row in df.itertuples():\n",
    "            self.data[counter] = {\n",
    "                'text': str(row[0]) + '.txt',\n",
    "                'audio': str(row[0]) + '.wav'\n",
    "            }\n",
    "            counter += 1\n",
    "        self.len = counter\n",
    "        self.only_char = only_char\n",
    "        del counter\n",
    "        del df\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        return self.audio_dir + self.data[index]['audio']\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        label_path = self.audio_dir + self.data[index]['text']\n",
    "        with open(label_path, 'r') as f:\n",
    "            label = clean_text(f.read()) if self.only_char else f.read() # Не учитывает ошибки в заполнение .txt\n",
    "        return label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self._get_audio_sample_path(idx)\n",
    "        text = self._get_audio_sample_label(idx)\n",
    "        \n",
    "        # this is the sample rate, which represents audio frequency. Whisper models are pretrained on a sample rate of 16000, so it's recommended not to change this value.\n",
    "        #audio, sample_rate = torchaudio.load(filepath)\n",
    "        audio, _ = librosa.load(filepath, sr=16000)\n",
    "        \n",
    "        tokenized = self.processor.tokenizer(\n",
    "            text, return_tensors='pt', padding='max_length', return_attention_mask=True, \n",
    "            max_length=model.config.max_length\n",
    "        )\n",
    "        \n",
    "        labels, attention_mask = tokenized['input_ids'][0], tokenized['attention_mask'][0]\n",
    "        \n",
    "        input_features = self.processor(audio, return_tensors=\"pt\", sampling_rate=16000).input_features[0]\n",
    "        \n",
    "        return [filepath, text, input_features, labels, attention_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.332146Z",
     "iopub.status.busy": "2023-09-04T16:51:33.331814Z",
     "iopub.status.idle": "2023-09-04T16:51:33.338088Z",
     "shell.execute_reply": "2023-09-04T16:51:33.337238Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.332116Z"
    }
   },
   "outputs": [],
   "source": [
    "# create train/val/test datasets\n",
    "train_dataset = WhisperDataset('../tatar_tts/train/', processor)\n",
    "eval_dataset = WhisperDataset('../tatar_tts/valid/', processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.340120Z",
     "iopub.status.busy": "2023-09-04T16:51:33.339329Z",
     "iopub.status.idle": "2023-09-04T16:51:33.348751Z",
     "shell.execute_reply": "2023-09-04T16:51:33.347701Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.340081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65891, 9691)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.350766Z",
     "iopub.status.busy": "2023-09-04T16:51:33.350192Z",
     "iopub.status.idle": "2023-09-04T16:51:33.360251Z",
     "shell.execute_reply": "2023-09-04T16:51:33.359326Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.350731Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, dataset: WhisperDataset) -> pd.DataFrame:\n",
    "    '''используется только для составления прогнозов по набору данных Whisper. \n",
    "    Он принимает в качестве входных данных модель и набор данных и возвращает прогнозы в виде панд.\n",
    "    Фрейм данных (т.е. таблица)'''\n",
    "    predicted_df = pd.DataFrame([], columns=['filename', 'pred', 'gt'])\n",
    "    for filepath, text, input_features, _, attention_mask in tqdm(dataset):\n",
    "        filename = filepath.replace('\\\\', '/').split('/')[-1]\n",
    "    \n",
    "        input_features = torch.stack([input_features]).to('cuda')\n",
    "        generated_ids = model.generate(inputs=input_features, attention_mask=attention_mask)\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "        predicted_df.loc[len(predicted_df)] = [filename, transcription, text]\n",
    "    return predicted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating current metric (not trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.366004Z",
     "iopub.status.busy": "2023-09-04T16:51:33.365667Z",
     "iopub.status.idle": "2023-09-04T16:51:50.491025Z",
     "shell.execute_reply": "2023-09-04T16:51:50.490066Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.365978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                | 145/9691 [00:58<57:38,  2.76it/s]"
     ]
    }
   ],
   "source": [
    "predicted_df = predict(model, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:50.493755Z",
     "iopub.status.busy": "2023-09-04T16:51:50.492660Z",
     "iopub.status.idle": "2023-09-04T16:51:50.505185Z",
     "shell.execute_reply": "2023-09-04T16:51:50.503916Z",
     "shell.execute_reply.started": "2023-09-04T16:51:50.493720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>pred</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eu.c7125b33-46dd-4dcd-b0dd-b4370a7d3272.wav</td>\n",
       "      <td>Садовая.</td>\n",
       "      <td>Садовая</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eu.518ad001-1e7f-4b1f-a03a-cb0e7ab213c0.wav</td>\n",
       "      <td>Центральное.</td>\n",
       "      <td>Центральная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eu.a610a47a-e238-44a9-bc51-a547f608ae12.wav</td>\n",
       "      <td>Центральное</td>\n",
       "      <td>Центральная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu.33d50dea-014e-4ced-8eca-fda21dcba219.wav</td>\n",
       "      <td>Революции.</td>\n",
       "      <td>Революции</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eu.1d1ff299-f454-4772-bfd9-680c2da6acd0.wav</td>\n",
       "      <td>Лесная</td>\n",
       "      <td>Лесная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eu.9a813c5e-4035-4170-b7fe-6324b76a38f7.wav</td>\n",
       "      <td>Зеленая</td>\n",
       "      <td>Зеленая</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eu.98a6a38a-07b0-4b00-bebb-cfa853a60186.wav</td>\n",
       "      <td>Молодежная</td>\n",
       "      <td>Молодежная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eu.e907120a-a234-41cf-853d-5c53897f094b.wav</td>\n",
       "      <td>Советская.</td>\n",
       "      <td>Советская</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eu.46de6638-3634-43f0-894d-03db84c2f763.wav</td>\n",
       "      <td>Советская.</td>\n",
       "      <td>Советская</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eu.ab9ecaf2-0185-4679-9327-3673d59bcc69.wav</td>\n",
       "      <td>Молодежная.</td>\n",
       "      <td>Молодежная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eu.b22a6650-9833-46e6-b093-fc824e1b1269.wav</td>\n",
       "      <td>Заречная.</td>\n",
       "      <td>Заречная</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       filename           pred           gt\n",
       "0   eu.c7125b33-46dd-4dcd-b0dd-b4370a7d3272.wav       Садовая.      Садовая\n",
       "1   eu.518ad001-1e7f-4b1f-a03a-cb0e7ab213c0.wav   Центральное.  Центральная\n",
       "2   eu.a610a47a-e238-44a9-bc51-a547f608ae12.wav    Центральное  Центральная\n",
       "3   eu.33d50dea-014e-4ced-8eca-fda21dcba219.wav     Революции.    Революции\n",
       "4   eu.1d1ff299-f454-4772-bfd9-680c2da6acd0.wav         Лесная       Лесная\n",
       "5   eu.9a813c5e-4035-4170-b7fe-6324b76a38f7.wav        Зеленая      Зеленая\n",
       "6   eu.98a6a38a-07b0-4b00-bebb-cfa853a60186.wav     Молодежная   Молодежная\n",
       "7   eu.e907120a-a234-41cf-853d-5c53897f094b.wav     Советская.    Советская\n",
       "8   eu.46de6638-3634-43f0-894d-03db84c2f763.wav     Советская.    Советская\n",
       "9   eu.ab9ecaf2-0185-4679-9327-3673d59bcc69.wav    Молодежная.   Молодежная\n",
       "10  eu.b22a6650-9833-46e6-b093-fc824e1b1269.wav      Заречная.     Заречная"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:50.507165Z",
     "iopub.status.busy": "2023-09-04T16:51:50.506665Z",
     "iopub.status.idle": "2023-09-04T16:51:50.516244Z",
     "shell.execute_reply": "2023-09-04T16:51:50.515073Z",
     "shell.execute_reply.started": "2023-09-04T16:51:50.507134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n"
     ]
    }
   ],
   "source": [
    "acc = sum((predicted_df['pred'] == predicted_df['gt'])) / len(predicted_df)\n",
    "print(f'{acc * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:50.518703Z",
     "iopub.status.busy": "2023-09-04T16:51:50.518089Z",
     "iopub.status.idle": "2023-09-04T16:51:50.527028Z",
     "shell.execute_reply": "2023-09-04T16:51:50.525487Z",
     "shell.execute_reply.started": "2023-09-04T16:51:50.518668Z"
    }
   },
   "outputs": [],
   "source": [
    "# function that transforms data after extracting it from the dataset. Here, the data_collate_fn simply reshapes the data.\n",
    "def data_collate_fn(data_list):\n",
    "    batch = len(data_list)\n",
    "    data_numpy = np.array(data_list)\n",
    "    input_features, labels, attention_mask = data_numpy[:, -3], data_numpy[:, -2], data_numpy[:, -1]\n",
    "    return {'input_features': torch.stack(input_features.tolist()),\n",
    "            'labels': torch.stack(labels.tolist()),\n",
    "            'attention_mask': torch.stack(attention_mask.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:50.530470Z",
     "iopub.status.busy": "2023-09-04T16:51:50.529776Z",
     "iopub.status.idle": "2023-09-04T16:51:50.546329Z",
     "shell.execute_reply": "2023-09-04T16:51:50.545397Z",
     "shell.execute_reply.started": "2023-09-04T16:51:50.530439Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TRAINING_ARGS,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:50.548169Z",
     "iopub.status.busy": "2023-09-04T16:51:50.547849Z",
     "iopub.status.idle": "2023-09-04T17:03:47.124973Z",
     "shell.execute_reply": "2023-09-04T17:03:47.124033Z",
     "shell.execute_reply.started": "2023-09-04T16:51:50.548141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_28/606901550.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_numpy = np.array(data_list)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='830' max='830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [830/830 11:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28/606901550.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_numpy = np.array(data_list)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=830, training_loss=0.002763278443290549, metrics={'train_runtime': 716.1745, 'train_samples_per_second': 2.304, 'train_steps_per_second': 1.159, 'total_flos': 4.76165910528e+17, 'train_loss': 0.002763278443290549, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:47.126953Z",
     "iopub.status.busy": "2023-09-04T17:03:47.126564Z",
     "iopub.status.idle": "2023-09-04T17:03:47.132825Z",
     "shell.execute_reply": "2023-09-04T17:03:47.131944Z",
     "shell.execute_reply.started": "2023-09-04T17:03:47.126921Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting folder of the newest checkpoint\n",
    "checkpoint_path = max(os.listdir(TRAINING_ARGS.output_dir), key=lambda x: int(x.split('-')[-1]) if 'checkpoint-' in x else 0)\n",
    "checkpoint_path = os.path.join(TRAINING_ARGS.output_dir, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:47.134928Z",
     "iopub.status.busy": "2023-09-04T17:03:47.134287Z",
     "iopub.status.idle": "2023-09-04T17:03:51.326360Z",
     "shell.execute_reply": "2023-09-04T17:03:51.325346Z",
     "shell.execute_reply.started": "2023-09-04T17:03:47.134896Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(checkpoint_path).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:51.328494Z",
     "iopub.status.busy": "2023-09-04T17:03:51.328121Z",
     "iopub.status.idle": "2023-09-04T17:03:54.221716Z",
     "shell.execute_reply": "2023-09-04T17:03:54.220635Z",
     "shell.execute_reply.started": "2023-09-04T17:03:51.328462Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 2/11 [00:00<00:02,  3.72it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 3/11 [00:00<00:02,  3.72it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 36%|███▋      | 4/11 [00:01<00:01,  3.67it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 5/11 [00:01<00:01,  3.79it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 55%|█████▍    | 6/11 [00:01<00:01,  3.87it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 64%|██████▎   | 7/11 [00:01<00:01,  3.86it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 8/11 [00:02<00:00,  3.82it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 9/11 [00:02<00:00,  3.83it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 10/11 [00:02<00:00,  3.86it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 11/11 [00:02<00:00,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_df = predict(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.223590Z",
     "iopub.status.busy": "2023-09-04T17:03:54.223138Z",
     "iopub.status.idle": "2023-09-04T17:03:54.234701Z",
     "shell.execute_reply": "2023-09-04T17:03:54.233633Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.223541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>pred</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eu.c7125b33-46dd-4dcd-b0dd-b4370a7d3272.wav</td>\n",
       "      <td>Садовая</td>\n",
       "      <td>Садовая</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eu.518ad001-1e7f-4b1f-a03a-cb0e7ab213c0.wav</td>\n",
       "      <td>Центральная</td>\n",
       "      <td>Центральная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eu.a610a47a-e238-44a9-bc51-a547f608ae12.wav</td>\n",
       "      <td>Центральная</td>\n",
       "      <td>Центральная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu.33d50dea-014e-4ced-8eca-fda21dcba219.wav</td>\n",
       "      <td>Революции</td>\n",
       "      <td>Революции</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eu.1d1ff299-f454-4772-bfd9-680c2da6acd0.wav</td>\n",
       "      <td>Лесная</td>\n",
       "      <td>Лесная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eu.9a813c5e-4035-4170-b7fe-6324b76a38f7.wav</td>\n",
       "      <td>Зеленая</td>\n",
       "      <td>Зеленая</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eu.98a6a38a-07b0-4b00-bebb-cfa853a60186.wav</td>\n",
       "      <td>Молодежная</td>\n",
       "      <td>Молодежная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eu.e907120a-a234-41cf-853d-5c53897f094b.wav</td>\n",
       "      <td>Советская</td>\n",
       "      <td>Советская</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eu.46de6638-3634-43f0-894d-03db84c2f763.wav</td>\n",
       "      <td>Советская</td>\n",
       "      <td>Советская</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eu.ab9ecaf2-0185-4679-9327-3673d59bcc69.wav</td>\n",
       "      <td>Молодежная</td>\n",
       "      <td>Молодежная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eu.b22a6650-9833-46e6-b093-fc824e1b1269.wav</td>\n",
       "      <td>Заречная</td>\n",
       "      <td>Заречная</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       filename         pred           gt\n",
       "0   eu.c7125b33-46dd-4dcd-b0dd-b4370a7d3272.wav      Садовая      Садовая\n",
       "1   eu.518ad001-1e7f-4b1f-a03a-cb0e7ab213c0.wav  Центральная  Центральная\n",
       "2   eu.a610a47a-e238-44a9-bc51-a547f608ae12.wav  Центральная  Центральная\n",
       "3   eu.33d50dea-014e-4ced-8eca-fda21dcba219.wav    Революции    Революции\n",
       "4   eu.1d1ff299-f454-4772-bfd9-680c2da6acd0.wav       Лесная       Лесная\n",
       "5   eu.9a813c5e-4035-4170-b7fe-6324b76a38f7.wav      Зеленая      Зеленая\n",
       "6   eu.98a6a38a-07b0-4b00-bebb-cfa853a60186.wav   Молодежная   Молодежная\n",
       "7   eu.e907120a-a234-41cf-853d-5c53897f094b.wav    Советская    Советская\n",
       "8   eu.46de6638-3634-43f0-894d-03db84c2f763.wav    Советская    Советская\n",
       "9   eu.ab9ecaf2-0185-4679-9327-3673d59bcc69.wav   Молодежная   Молодежная\n",
       "10  eu.b22a6650-9833-46e6-b093-fc824e1b1269.wav     Заречная     Заречная"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.236945Z",
     "iopub.status.busy": "2023-09-04T17:03:54.236278Z",
     "iopub.status.idle": "2023-09-04T17:03:54.245320Z",
     "shell.execute_reply": "2023-09-04T17:03:54.244367Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.236910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# metric: accuracy\n",
    "acc = sum((predicted_df['pred'] == predicted_df['gt'])) / len(predicted_df)\n",
    "print(f'{acc * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom sample test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.247339Z",
     "iopub.status.busy": "2023-09-04T17:03:54.246977Z",
     "iopub.status.idle": "2023-09-04T17:03:54.257558Z",
     "shell.execute_reply": "2023-09-04T17:03:54.256699Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.247307Z"
    }
   },
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(os.path.join(DATASET_DIR, 'sample/eu.0124f456-13b8-4765-936a-36bfd483683e.wav'), sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.259598Z",
     "iopub.status.busy": "2023-09-04T17:03:54.259174Z",
     "iopub.status.idle": "2023-09-04T17:03:54.319455Z",
     "shell.execute_reply": "2023-09-04T17:03:54.318206Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.259545Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = processor(audio, return_tensors='pt', sampling_rate=sample_rate)\n",
    "input_features = inputs.input_features.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.321383Z",
     "iopub.status.busy": "2023-09-04T17:03:54.321037Z",
     "iopub.status.idle": "2023-09-04T17:03:54.506578Z",
     "shell.execute_reply": "2023-09-04T17:03:54.505617Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.321352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(inputs=input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.508427Z",
     "iopub.status.busy": "2023-09-04T17:03:54.508090Z",
     "iopub.status.idle": "2023-09-04T17:03:54.516341Z",
     "shell.execute_reply": "2023-09-04T17:03:54.515232Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.508396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новая\n"
     ]
    }
   ],
   "source": [
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
