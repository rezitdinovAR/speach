{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade comet_ml -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:38.118293Z",
     "iopub.status.busy": "2023-09-04T16:50:38.117992Z",
     "iopub.status.idle": "2023-09-04T16:50:50.571885Z",
     "shell.execute_reply": "2023-09-04T16:50:50.570848Z",
     "shell.execute_reply.started": "2023-09-04T16:50:38.118267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-07 13:32:56,618] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHISPER_MODEL examples:\n",
    "\n",
    "* 1) openai/whisper-small (suitable for testing functionality, not very accurate on sentences, but capable of recognizing individual words or phrases. Requires low computational resources)\n",
    "* 2) openai/whisper-medium (recommended medium model)\n",
    "* 3) openai/whisper-large (sufficiently accurate on large sentences, but requires significant computational resources)\n",
    "* 4) openai/whisper-large-v2 (sufficiently accurate on large sentences, but requires significant computational resources)\n",
    "* 5) lorenzoncina/whisper-medium-ru (a model finetuned on the Russian language - recommended for training on Russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:50.574473Z",
     "iopub.status.busy": "2023-09-04T16:50:50.573546Z",
     "iopub.status.idle": "2023-09-04T16:50:50.615610Z",
     "shell.execute_reply": "2023-09-04T16:50:50.614730Z",
     "shell.execute_reply.started": "2023-09-04T16:50:50.574437Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
    "\n",
    "WHISPER_MODEL = 'openai/whisper-small'\n",
    "DATASET_DIR = '/kaggle/input/it-spectrum-dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:50:50.619311Z",
     "iopub.status.busy": "2023-09-04T16:50:50.619037Z",
     "iopub.status.idle": "2023-09-04T16:51:32.900398Z",
     "shell.execute_reply": "2023-09-04T16:51:32.899357Z",
     "shell.execute_reply.started": "2023-09-04T16:50:50.619288Z"
    }
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(WHISPER_MODEL)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(WHISPER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:32.904246Z",
     "iopub.status.busy": "2023-09-04T16:51:32.903878Z",
     "iopub.status.idle": "2023-09-04T16:51:32.909408Z",
     "shell.execute_reply": "2023-09-04T16:51:32.908477Z",
     "shell.execute_reply.started": "2023-09-04T16:51:32.904212Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the model's language and defining the task of transcription\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"tatar\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training dataset. When indexed, it returns a list containing:\n",
    "\n",
    "* 1) filepath - path to the audio\n",
    "* 2) text - transcribed text by annotators\n",
    "* 3) input_features - audio features for prediction\n",
    "* 4) labels - transcribed text by annotators converted into tokens\n",
    "* 5) attention mask - an attention mask where each element indicates whether the model should pay attention to the token corresponding to the same index in the labels list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.307664Z",
     "iopub.status.busy": "2023-09-04T16:51:33.307288Z",
     "iopub.status.idle": "2023-09-04T16:51:33.317645Z",
     "shell.execute_reply": "2023-09-04T16:51:33.316636Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.307616Z"
    }
   },
   "outputs": [],
   "source": [
    "class WhisperDataset(Dataset):\n",
    "    def __init__(self, audio_dir: str, processor, max_length, only_char=True):\n",
    "        self.audio_dir = audio_dir\n",
    "        df = pd.read_csv(audio_dir[:-1] + '.csv', index_col='id')\n",
    "        self.data = {}\n",
    "        counter = 0\n",
    "        for row in df.itertuples():\n",
    "            if not os.path.exists(audio_dir + str(row[0]) + '.txt'):\n",
    "                print('Отсутствует файл', str(row[0]) + '.txt')\n",
    "                continue\n",
    "            if not os.path.exists(audio_dir + str(row[0]) + '.wav'):\n",
    "                print(f'Отсутствует файл', str(row[0]) + '.wav')\n",
    "                continue\n",
    "            self.data[counter] = {\n",
    "                'text': str(row[0]) + '.txt',\n",
    "                'audio': str(row[0]) + '.wav'\n",
    "            }\n",
    "            counter += 1\n",
    "        self.len = counter - 1\n",
    "        self.only_char = only_char\n",
    "        del counter\n",
    "        del df\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        return self.audio_dir + self.data[index]['audio']\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        label_path = self.audio_dir + self.data[index]['text']\n",
    "        with open(label_path, 'r') as f:\n",
    "            label = clean_text(f.read()) if self.only_char else f.read() # Не учитывает ошибки в заполнение .txt\n",
    "        return label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self._get_audio_sample_path(idx)\n",
    "        text = self._get_audio_sample_label(idx)\n",
    "        \n",
    "        audio, sample_rate = torchaudio.load(filepath)\n",
    "        audio = torch.reshape(audio, (-1,))\n",
    "        \n",
    "        tokenized = self.processor.tokenizer(\n",
    "            text, return_tensors='pt', padding='max_length', return_attention_mask=True, \n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        labels, attention_mask = tokenized['input_ids'][0], tokenized['attention_mask'][0]\n",
    "        \n",
    "        input_features = self.processor(audio, return_tensors=\"pt\", sampling_rate=sample_rate).input_features[0]\n",
    "        \n",
    "        return {\n",
    "            'input_features': input_features, \n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.332146Z",
     "iopub.status.busy": "2023-09-04T16:51:33.331814Z",
     "iopub.status.idle": "2023-09-04T16:51:33.338088Z",
     "shell.execute_reply": "2023-09-04T16:51:33.337238Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.332116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отсутствует файл 294.38.txt\n",
      "Отсутствует файл 241.2.txt\n",
      "Отсутствует файл 295.8.txt\n",
      "Отсутствует файл 241.3.txt\n",
      "Отсутствует файл 224.3.txt\n",
      "Отсутствует файл 227.2.txt\n",
      "Отсутствует файл 298.7.txt\n",
      "Отсутствует файл 282.16.txt\n",
      "Отсутствует файл 217.3.txt\n",
      "Отсутствует файл 299.1.txt\n",
      "Отсутствует файл 272.11.txt\n",
      "Отсутствует файл 297.16.txt\n",
      "Отсутствует файл 294.5.txt\n",
      "Отсутствует файл 298.26.txt\n",
      "Отсутствует файл 293.26.txt\n",
      "Отсутствует файл 293.28.txt\n",
      "Отсутствует файл 292.26.txt\n",
      "Отсутствует файл 272.51.txt\n",
      "Отсутствует файл 280.47.txt\n",
      "Отсутствует файл 228.2.txt\n",
      "Отсутствует файл 296.1.txt\n",
      "Отсутствует файл 295.18.txt\n",
      "Отсутствует файл 297.11.txt\n",
      "Отсутствует файл 284.9.txt\n",
      "Отсутствует файл 227.3.txt\n",
      "Отсутствует файл 224.2.txt\n",
      "Отсутствует файл 299.1.txt\n",
      "Отсутствует файл 295.11.txt\n",
      "Отсутствует файл 238.2.txt\n",
      "Отсутствует файл 228.3.txt\n",
      "Отсутствует файл 238.3.txt\n",
      "Отсутствует файл 202.3.txt\n",
      "Отсутствует файл 288.38.txt\n",
      "Отсутствует файл 292.24.txt\n",
      "Отсутствует файл 291.18.txt\n",
      "Отсутствует файл 295.21.txt\n",
      "Отсутствует файл 290.36.txt\n",
      "Отсутствует файл 202.2.txt\n",
      "Отсутствует файл 222.3.txt\n"
     ]
    }
   ],
   "source": [
    "# create train/val/test datasets\n",
    "train_dataset = WhisperDataset('../tatar_asr_2/train/', processor, model.config.max_length)\n",
    "valid_dataset = WhisperDataset('../tatar_asr_2/valid/', processor, model.config.max_length)\n",
    "test_dataset = WhisperDataset('../tatar_asr_1/valid/', processor, model.config.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T16:51:33.340120Z",
     "iopub.status.busy": "2023-09-04T16:51:33.339329Z",
     "iopub.status.idle": "2023-09-04T16:51:33.348751Z",
     "shell.execute_reply": "2023-09-04T16:51:33.347701Z",
     "shell.execute_reply.started": "2023-09-04T16:51:33.340081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100235, 10446)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  7 13:42:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    25W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    27W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml.init( project_name = \"TatAsr-whisper\", experiment_name = \"TatAsr-whisper-dataset-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    global model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./whisper-dataset-2', \n",
    "        overwrite_output_dir=True, \n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=7,\n",
    "        save_steps=500, \n",
    "        save_total_limit=2,\n",
    "        do_train=True,\n",
    "    )\n",
    "    \n",
    "    set_seed(42)\n",
    "    torch.manual_seed(7)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m You are trying to log string value as a metric. This is not recommended.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/gumaonelove/tatasr-whisper/c74f48a9a0774a179aa03ed2a82b0e39\n",
      "\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7120' max='7160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7120/7160 3:32:42 < 01:11, 0.56 it/s, Epoch 0.99/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_launcher(training_function, num_processes=2, mixed_precision='fp16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:47.126953Z",
     "iopub.status.busy": "2023-09-04T17:03:47.126564Z",
     "iopub.status.idle": "2023-09-04T17:03:47.132825Z",
     "shell.execute_reply": "2023-09-04T17:03:47.131944Z",
     "shell.execute_reply.started": "2023-09-04T17:03:47.126921Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting folder of the newest checkpoint\n",
    "checkpoint_path = max(os.listdir('./whisper-dataset-2'), key=lambda x: int(x.split('-')[-1]) if 'checkpoint-' in x else 0)\n",
    "checkpoint_path = os.path.join('./whisper', checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:47.134928Z",
     "iopub.status.busy": "2023-09-04T17:03:47.134287Z",
     "iopub.status.idle": "2023-09-04T17:03:51.326360Z",
     "shell.execute_reply": "2023-09-04T17:03:51.325346Z",
     "shell.execute_reply.started": "2023-09-04T17:03:47.134896Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset: WhisperDataset) -> pd.DataFrame:\n",
    "    predicted_df = pd.DataFrame([], columns=['filename', 'pred', 'gt'])\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        item = dataset[idx]\n",
    "        filepath = dataset._get_audio_sample_path(idx)\n",
    "        text = dataset._get_audio_sample_label(idx)\n",
    "        input_features = item['input_features']\n",
    "        attention_mask = item['attention_mask']\n",
    "        filename = filepath.replace('\\\\', '/').split('/')[-1]\n",
    "        model = model.to('cuda')\n",
    "    \n",
    "        input_features = torch.stack([input_features]).to('cuda')\n",
    "        generated_ids = model.generate(inputs=input_features, attention_mask=attention_mask)\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "        predicted_df.loc[len(predicted_df)] = [filename, transcription, text]\n",
    "    return predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:51.328494Z",
     "iopub.status.busy": "2023-09-04T17:03:51.328121Z",
     "iopub.status.idle": "2023-09-04T17:03:54.221716Z",
     "shell.execute_reply": "2023-09-04T17:03:54.220635Z",
     "shell.execute_reply.started": "2023-09-04T17:03:51.328462Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14213/14213 [1:20:53<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_df = predict(model, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.223590Z",
     "iopub.status.busy": "2023-09-04T17:03:54.223138Z",
     "iopub.status.idle": "2023-09-04T17:03:54.234701Z",
     "shell.execute_reply": "2023-09-04T17:03:54.233633Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.223541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>pred</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>282.311.wav</td>\n",
       "      <td>бүген</td>\n",
       "      <td>бүген</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.2.wav</td>\n",
       "      <td>ниһаять халисә үзен кулга алды плащын һәм шарф...</td>\n",
       "      <td>ниһаять халисә үзен кулга алды плащын һәм шарф...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272.354.wav</td>\n",
       "      <td>бөгелеп төшмәве</td>\n",
       "      <td>бөгелеп төшмәве</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>270.262.wav</td>\n",
       "      <td>бар уңышка ирешкән командалар</td>\n",
       "      <td>бар уңышка ирешкән командалар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276.116.wav</td>\n",
       "      <td>ураза кешенең сәламәтлегенә зыян салса шулай у...</td>\n",
       "      <td>ураза кешенең сәламәтлегенә зыян салса шулай у...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14208</th>\n",
       "      <td>290.661.wav</td>\n",
       "      <td>туктале</td>\n",
       "      <td>туктале</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14209</th>\n",
       "      <td>273.514.wav</td>\n",
       "      <td>ярый ярый һич һаваланма</td>\n",
       "      <td>ярый ярый һич хафаланма</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14210</th>\n",
       "      <td>281.647.wav</td>\n",
       "      <td>евролигедән</td>\n",
       "      <td>евролигадан</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14211</th>\n",
       "      <td>290.133.wav</td>\n",
       "      <td>чәбәкли чәбәкли йөгереп килеп</td>\n",
       "      <td>чәбәкли чәбәкли йөгереп килеп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14212</th>\n",
       "      <td>237.2.wav</td>\n",
       "      <td>җиһангәр капкадан акрын гына керде дә йорт урт...</td>\n",
       "      <td>җиһангир капкадан акрын гына керде дә йорт урт...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14213 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                               pred  \\\n",
       "0      282.311.wav                                              бүген   \n",
       "1         20.2.wav  ниһаять халисә үзен кулга алды плащын һәм шарф...   \n",
       "2      272.354.wav                                    бөгелеп төшмәве   \n",
       "3      270.262.wav                      бар уңышка ирешкән командалар   \n",
       "4      276.116.wav  ураза кешенең сәламәтлегенә зыян салса шулай у...   \n",
       "...            ...                                                ...   \n",
       "14208  290.661.wav                                            туктале   \n",
       "14209  273.514.wav                            ярый ярый һич һаваланма   \n",
       "14210  281.647.wav                                        евролигедән   \n",
       "14211  290.133.wav                      чәбәкли чәбәкли йөгереп килеп   \n",
       "14212    237.2.wav  җиһангәр капкадан акрын гына керде дә йорт урт...   \n",
       "\n",
       "                                                      gt  \n",
       "0                                                  бүген  \n",
       "1      ниһаять халисә үзен кулга алды плащын һәм шарф...  \n",
       "2                                        бөгелеп төшмәве  \n",
       "3                          бар уңышка ирешкән командалар  \n",
       "4      ураза кешенең сәламәтлегенә зыян салса шулай у...  \n",
       "...                                                  ...  \n",
       "14208                                            туктале  \n",
       "14209                            ярый ярый һич хафаланма  \n",
       "14210                                        евролигадан  \n",
       "14211                      чәбәкли чәбәкли йөгереп килеп  \n",
       "14212  җиһангир капкадан акрын гына керде дә йорт урт...  \n",
       "\n",
       "[14213 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.236945Z",
     "iopub.status.busy": "2023-09-04T17:03:54.236278Z",
     "iopub.status.idle": "2023-09-04T17:03:54.245320Z",
     "shell.execute_reply": "2023-09-04T17:03:54.244367Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.236910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.153099275311334%\n"
     ]
    }
   ],
   "source": [
    "# metric: accuracy\n",
    "acc = sum((predicted_df['pred'] == predicted_df['gt'])) / len(predicted_df)\n",
    "print(f'{acc * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom sample test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.247339Z",
     "iopub.status.busy": "2023-09-04T17:03:54.246977Z",
     "iopub.status.idle": "2023-09-04T17:03:54.257558Z",
     "shell.execute_reply": "2023-09-04T17:03:54.256699Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.247307Z"
    }
   },
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(os.path.join(DATASET_DIR, 'sample/eu.0124f456-13b8-4765-936a-36bfd483683e.wav'), sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.259598Z",
     "iopub.status.busy": "2023-09-04T17:03:54.259174Z",
     "iopub.status.idle": "2023-09-04T17:03:54.319455Z",
     "shell.execute_reply": "2023-09-04T17:03:54.318206Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.259545Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = processor(audio, return_tensors='pt', sampling_rate=sample_rate)\n",
    "input_features = inputs.input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.321383Z",
     "iopub.status.busy": "2023-09-04T17:03:54.321037Z",
     "iopub.status.idle": "2023-09-04T17:03:54.506578Z",
     "shell.execute_reply": "2023-09-04T17:03:54.505617Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.321352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(inputs=input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T17:03:54.508427Z",
     "iopub.status.busy": "2023-09-04T17:03:54.508090Z",
     "iopub.status.idle": "2023-09-04T17:03:54.516341Z",
     "shell.execute_reply": "2023-09-04T17:03:54.515232Z",
     "shell.execute_reply.started": "2023-09-04T17:03:54.508396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новая\n"
     ]
    }
   ],
   "source": [
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
