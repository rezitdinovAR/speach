{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc027552",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "929113a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import comet_ml\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset, ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import gc\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec5a2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4176dd73062b437e8b923b79f482c6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c42eaf811704b9ba9e5f2ad523d089d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2328d304d9fd4a319b99c8e5b951f804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_dir='./', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9862b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b5128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03fdbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>» Ачуы килсә, ни эшләргә уйлагандыр, анысын сәхия әйтеп бетермәде, һич көтмәгәндә күкле-яшелле тавыш белән тыштагыларга җырлап җавап кайтарды: әллә, дустым, укыдыңмы Каргалар курсысында; Карга кебек карылдыйсың Тәрәзә турысында.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"дигән сорауга каршы тавыш бирүгә чакырып, республиканы төрле плакат-листовкалар өеме эчендә калдырдылар.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Кояшта тән пешкәндә Бер аш кашыгы үсемлек мае, ике аш кашыгы каймак һәм бер йомырка сарысы кирәк.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Алар мине “өф” ләп үстермәде, Булсын диеп безгә яраклы.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Шуннан соң үзенең чыгышында Русия җәмәгать пулатының комиссия башлыгы бүгенге җәмгыятьтәге әхлакый нормаларга таянырга кирәклеге турында сөйләде.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Симез Мәче йортның юлларын белә иде, тиз бер якка сыенды.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>әллә кайчан кулын кыскартырга иде инде,.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>әмма председательлекне дә кулына алган секретарьлар кемнәр алар, барысы да лаек кешеләрме?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Судан моны израильнең хәрби операциясе булмагае дип шикләнә.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Шуның белән җәләл хәзрәт ал да гөл булыр.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eafb9a",
   "metadata": {},
   "source": [
    "## Causal Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07edacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13640dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82382df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tatar_tokenizer/tokenizer_config.json',\n",
       " 'tatar_tokenizer/special_tokens_map.json',\n",
       " 'tatar_tokenizer/vocab.json',\n",
       " 'tatar_tokenizer/merges.txt',\n",
       " 'tatar_tokenizer/added_tokens.json',\n",
       " 'tatar_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(dataset, 52_000, length=1024)\n",
    "tokenizer.save_pretrained(\"tatar_tokenizer-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06de850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "446d4bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271a4f3f91f141e6816056963ad00f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/400828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1588 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1732 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1170 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2516 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1119 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd6fd4da9794daa89757d0cf623c0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1110 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1147 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1354 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1342 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=8, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8d9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a65a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e56417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca7c618e8f549b18412d62a6f845836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/400828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4ad43404e146e3a408ef39c7e8a059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04efaf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 400828\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 100207\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b3f595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ртына мин, иң беренче чиратта, нәрсә тәкъдим итәр идем? Мәсьәләне спон�'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e6e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d1b1e",
   "metadata": {},
   "source": [
    "### GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a22ad258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db3cb725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 15 13:25:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    25W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    27W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5756667",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4106f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml.init( project_name = \"TatNlp\", experiment_name = \"TatNlp-distill-gp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7605a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    training_args = TrainingArguments(\n",
    "        f\"{model_name}-finetuned-tatar_nlp_1\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        overwrite_output_dir=True, \n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        save_steps=500, \n",
    "        save_total_limit=2,\n",
    "        do_train=True,\n",
    "    )\n",
    "    set_seed(42)\n",
    "    torch.manual_seed(7)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"test\"],\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da04605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m You are trying to log string value as a metric. This is not recommended.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/gumaonelove/tatnlp/9344357af5074c9c9fb969f019781ce1\n",
      "\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9851' max='9850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9850/9850 2:44:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.992200</td>\n",
       "      <td>0.878971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.881900</td>\n",
       "      <td>0.822710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.845300</td>\n",
       "      <td>0.797918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>0.786087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.782212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown offline streaming error: OSError(28, 'No space left on device'). Check logs for details.\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /home/asr/projects/speach/research/NLP/distilgpt2-finetuned-tatar_nlp_1/checkpoint-9500/optimizer.pt for uploading.\n",
      "Please double-check the file path and permissions\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /home/asr/projects/speach/research/NLP/distilgpt2-finetuned-tatar_nlp_1/checkpoint-9500/rng_state_0.pth for uploading.\n",
      "Please double-check the file path and permissions\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /home/asr/projects/speach/research/NLP/distilgpt2-finetuned-tatar_nlp_1/checkpoint-9500/trainer_state.json for uploading.\n",
      "Please double-check the file path and permissions\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /home/asr/projects/speach/research/NLP/distilgpt2-finetuned-tatar_nlp_1/checkpoint-9500/scheduler.pt for uploading.\n",
      "Please double-check the file path and permissions\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /home/asr/projects/speach/research/NLP/distilgpt2-finetuned-tatar_nlp_1/checkpoint-9500/config.json for uploading.\n",
      "Please double-check the file path and permissions\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /home/asr/projects/speach/research/NLP/distilgpt2-finetuned-tatar_nlp_1/checkpoint-9500/training_args.bin for uploading.\n",
      "Please double-check the file path and permissions\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /home/asr/projects/speach/research/NLP/distilgpt2-finetuned-tatar_nlp_1/checkpoint-9500/pytorch_model.bin for uploading.\n",
      "Please double-check the file path and permissions\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/gumaonelove/tatnlp/9344357af5074c9c9fb969f019781ce1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch [25]                  : (0.25, 5.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_loss [5]               : (0.7822118401527405, 0.8789709806442261)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_runtime [5]            : (166.2534, 166.7316)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_samples_per_second [5] : (754.608, 756.779)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_steps_per_second [5]   : (47.166, 47.301)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate [19]          : (1.7766497461928936e-06, 4.746192893401015e-05)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [1004]                 : (0.7850627303123474, 4.224012851715088)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     total_flos                  : 8.23607357865984e+16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss                  : 0.8990083181313452\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_runtime               : 10042.9061\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_samples_per_second    : 251.009\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_steps_per_second      : 0.981\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : TatNlp-distill-gp2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_frozen                            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_n_gpu                             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_no_sync_in_gradient_accumulation  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_setup_devices                     : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adafactor                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta1                         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta2                         : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_epsilon                       : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/auto_find_batch_size               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16_full_eval                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/data_seed                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_drop_last               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_num_workers             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_pin_memory              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_backend                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_broadcast_buffers              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_bucket_cap_mb                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_find_unused_parameters         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout                        : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout_delta                  : 0:30:00\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/debug                              : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed_plugin                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/default_optim                      : adamw_torch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/device                             : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/disable_tqdm                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dispatch_batches                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/distributed_state                  : Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_eval                            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_predict                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_train                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_accumulation_steps            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_batch_size                    : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_delay                         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_steps                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/evaluation_strategy                : IntervalStrategy.EPOCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_backend                       : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_full_eval                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_opt_level                     : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/framework                          : pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp                               : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_config                        : {'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_min_num_params                : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_transformer_layer_cls_to_wrap : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/full_determinism                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_accumulation_steps        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/greater_is_better                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/group_by_length                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/half_precision_backend             : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_always_push                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_model_id                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_private_repo                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_strategy                       : HubStrategy.EVERY_SAVE\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_token                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ignore_data_skip                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_inputs_for_metrics         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/jit_mode_eval                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_names                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_smoothing_factor             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/learning_rate                      : 5e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/length_column_name                 : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/load_best_model_at_end             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_process_index                : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_rank                         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level                          : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level_replica                  : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_on_each_node                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_dir                        : distilgpt2-finetuned-tatar_nlp_1/runs/Sep15_13-25-54_msteam\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_first_step                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_nan_inf_filter             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_steps                      : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_strategy                   : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_type                  : SchedulerType.LINEAR\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_grad_norm                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_steps                          : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/metric_for_best_model              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/mp_parameters                      : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/n_gpu                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/no_cuda                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/num_train_epochs                   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim                              : OptimizerNames.ADAMW_TORCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_args                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/output_dir                         : distilgpt2-finetuned-tatar_nlp_1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/overwrite_output_dir               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/parallel_mode                      : ParallelMode.DISTRIBUTED\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/past_index                         : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_eval_batch_size         : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_train_batch_size        : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_eval_batch_size            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_train_batch_size           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/place_model_on_device              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/prediction_loss_only               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/process_index                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_model_id               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_organization           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_token                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ray_scope                          : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/remove_unused_columns              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/report_to                          : ['comet_ml']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/resume_from_checkpoint             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/run_name                           : distilgpt2-finetuned-tatar_nlp_1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_on_each_node                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_safetensors                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_steps                         : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_strategy                      : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_total_limit                   : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/seed                               : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/sharded_ddp                        : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_log                         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_save                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/skip_memory_metrics                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tf32                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_backend              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_mode                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torchdynamo                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_metrics_debug                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_num_cores                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/train_batch_size                   : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_cpu                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_ipex                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_legacy_prediction_loop         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_mps_device                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_ratio                       : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_steps                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/weight_decay                       : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/world_size                         : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_auto_class                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_commit_hash                     : 38cc92ec43315abd5136313225e95acc5986876c\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_name_or_path                    : distilgpt2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_num_labels                      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/activation_function              : gelu_new\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/add_cross_attention              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/architectures                    : ['GPT2LMHeadModel']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attn_pdrop                       : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attribute_map                    : {'hidden_size': 'n_embd', 'max_position_embeddings': 'n_positions', 'num_attention_heads': 'n_head', 'num_hidden_layers': 'n_layer'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bad_words_ids                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/begin_suppress_tokens            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bos_token_id                     : 50256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/chunk_size_feed_forward          : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/cross_attention_hidden_size      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_start_token_id           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/diversity_penalty                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/do_sample                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/early_stopping                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/embd_pdrop                       : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_no_repeat_ngram_size     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/eos_token_id                     : 50256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/exponential_decay_length_penalty : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/finetuning_task                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_bos_token_id              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_eos_token_id              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/id2label                         : {0: 'LABEL_0'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/initializer_range                : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_composition                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_decoder                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_encoder_decoder               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/keys_to_ignore_at_inference      : ['past_key_values']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/label2id                         : {'LABEL_0': 0}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/layer_norm_epsilon               : 1e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/length_penalty                   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_length                       : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/min_length                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/model_type                       : gpt2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_ctx                            : 1024\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_embd                           : 768\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_head                           : 12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_inner                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_layer                          : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_positions                      : 1024\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/name_or_path                     : distilgpt2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/no_repeat_ngram_size             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beam_groups                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beams                        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_labels                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_return_sequences             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_attentions                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_hidden_states             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_scores                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pad_token_id                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/prefix                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/problem_type                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pruned_heads                     : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/remove_invalid_values            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/reorder_and_upcast_attn          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/repetition_penalty               : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/resid_pdrop                      : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict                      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict_in_generate          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/scale_attn_by_inverse_layer_idx  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/scale_attn_weights               : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sep_token_id                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/summary_activation               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/summary_first_dropout            : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/summary_proj_to_labels           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/summary_type                     : cls_index\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/summary_use_proj                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/suppress_tokens                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/task_specific_params             : {'text-generation': {'do_sample': True, 'max_length': 50}}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/temperature                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tf_legacy_loss                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_encoder_decoder              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_word_embeddings              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tokenizer_class                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_k                            : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_p                            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torch_dtype                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torchscript                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/transformers_version             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/typical_p                        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_bfloat16                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_cache                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_return_dict                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_size                       : 50257\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 11 (937.55 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
      "Exception in thread Thread-55:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asr/miniconda3/envs/asr/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/asr/miniconda3/envs/asr/lib/python3.11/site-packages/comet_ml/comet.py\", line 224, in run\n",
      "    self._after_run()\n",
      "  File \"/home/asr/miniconda3/envs/asr/lib/python3.11/site-packages/comet_ml/comet.py\", line 1097, in _after_run\n",
      "    self.file.close()\n",
      "OSError: [Errno 28] No space left on device\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 file(s), remaining 932.56 MB/937.61 MB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 923.76 MB/937.61 MB, Throughput 600.32 KB/s, ETA ~1576s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 916.84 MB/937.61 MB, Throughput 472.36 KB/s, ETA ~1988s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 910.26 MB/937.61 MB, Throughput 448.88 KB/s, ETA ~2077s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 900.91 MB/937.61 MB, Throughput 638.70 KB/s, ETA ~1445s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 893.22 MB/937.61 MB, Throughput 524.59 KB/s, ETA ~1744s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 885.04 MB/937.61 MB, Throughput 558.19 KB/s, ETA ~1624s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 877.74 MB/937.61 MB, Throughput 497.94 KB/s, ETA ~1806s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 869.41 MB/937.61 MB, Throughput 568.86 KB/s, ETA ~1566s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 861.73 MB/937.61 MB, Throughput 524.08 KB/s, ETA ~1684s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 851.55 MB/937.61 MB, Throughput 694.13 KB/s, ETA ~1257s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 841.44 MB/937.61 MB, Throughput 689.84 KB/s, ETA ~1250s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 831.48 MB/937.61 MB, Throughput 680.26 KB/s, ETA ~1252s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 821.89 MB/937.61 MB, Throughput 654.15 KB/s, ETA ~1287s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 814.18 MB/937.61 MB, Throughput 526.20 KB/s, ETA ~1585s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 804.42 MB/937.61 MB, Throughput 665.86 KB/s, ETA ~1238s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 794.36 MB/937.61 MB, Throughput 686.67 KB/s, ETA ~1185s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m notebook_launcher(training_function, num_processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mixed_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/accelerate/launchers.py:154\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_processes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     start_processes(launcher, args\u001b[38;5;241m=\u001b[39margs, nprocs\u001b[38;5;241m=\u001b[39mnum_processes, start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mjoin():\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:109\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m ready \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinels\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m    111\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m error_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentinel \u001b[38;5;129;01min\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = notebook_launcher(training_function, num_processes=2, mixed_precision='fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d63eb78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mexp(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43dee9",
   "metadata": {},
   "source": [
    "### Pipeline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fa2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    \"Rocketknight1/distilgpt2-finetuned-wikitext2\",\n",
    "    framework=\"tf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b32bed",
   "metadata": {},
   "source": [
    "## Masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e49ae6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilroberta-base\"\n",
    "BATCH_SIZE = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b9d488c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6f21898fb84dab8937765449009c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8663d9417dfa40e5b231c50fbb6f3820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c0f4de7b58418aaea8b3b848db8ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed86d014137e4b659852ef4904ce40bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tatar_tokenizer-distill-bert/tokenizer_config.json',\n",
       " 'tatar_tokenizer-distill-bert/special_tokens_map.json',\n",
       " 'tatar_tokenizer-distill-bert/vocab.json',\n",
       " 'tatar_tokenizer-distill-bert/merges.txt',\n",
       " 'tatar_tokenizer-distill-bert/added_tokens.json',\n",
       " 'tatar_tokenizer-distill-bert/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(dataset, 52_000, length=1024)\n",
    "tokenizer.save_pretrained(\"tatar_tokenizer-distill-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "494dd26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61415af7d0dd43568c64020c9de94e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/400828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (960 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f5049d6c7f4541a0783d509330120c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (935 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=8, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40e13cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108912216bd94d0082da14b9bf57bff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/400828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a691830d9f43e58d3f914173dd5e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fbf9ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d7aeb8be2e4e0cbf814472bab4898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67ae679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml.init( project_name = \"TatNlp\", experiment_name = \"TatNlp-distill-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8718e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    training_args = TrainingArguments(\n",
    "        f\"{model_name}-finetuned-tatar_nlp_2\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        overwrite_output_dir=True, \n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        save_steps=500, \n",
    "        save_total_limit=2,\n",
    "        do_train=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "    set_seed(42)\n",
    "    torch.manual_seed(7)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m You are trying to log string value as a metric. This is not recommended.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/gumaonelove/tatnlp/61a822b5e1ac47489ee22194071faef0\n",
      "\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='6655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 204/6655 04:09 < 2:12:59, 0.81 it/s, Epoch 0.15/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='6655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 204/6655 04:09 < 2:12:59, 0.81 it/s, Epoch 0.15/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = notebook_launcher(training_function, num_processes=2, mixed_precision='fp16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915e8ba",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# You can of course use your own model checkpoint here instead of mine\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", \n",
    "    \"Rocketknight1/distilroberta-base-finetuned-wikitext2\",\n",
    "    framework=\"tf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423874f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filler(\"The most common household pets are <mask> and dogs.\", top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662df568",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filler(\"The Gulf War was a conflict that took place in <mask> in 1990-1991.\", top_k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
